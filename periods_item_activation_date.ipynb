{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing a simple model based solely on features generated from item activation date. We separate this model since this data is available for all the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import featurize_date_col, TpotAutoml\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "sns.set()\n",
    "SEED = 13\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', usecols=['item_id', 'deal_probability', 'activation_date'],\n",
    "                   parse_dates=['activation_date'], infer_datetime_format=True)\n",
    "test = pd.read_csv('data/test.csv', usecols=['item_id', 'activation_date'],\n",
    "                  parse_dates=['activation_date'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.rename(columns={'activation_date': 'item_activation_date'})\n",
    "train = featurize_date_col(train, 'item_activation_date', remove_when_done=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.set_index('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'deal_probability'\n",
    "SCORING = 'r2'\n",
    "X = (train.drop(target, axis=1)).values\n",
    "y = train[target].values\n",
    "tss = TimeSeriesSplit(n_splits=4)\n",
    "train_index, test_index = list(tss.split(X))[-1]\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import config_dict\n",
    "base_model = RandomForestRegressor()\n",
    "model = RandomizedSearchCV(estimator=base_model, random_state=SEED,\n",
    "                          param_distributions=config_dict['sklearn.ensemble.RandomForestRegressor'],\n",
    "                         n_iter=7,\n",
    "                         scoring='r2',\n",
    "                         cv=TimeSeriesSplit(n_splits=4),\n",
    "                         verbose=2,\n",
    "                         n_jobs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 7 candidates, totalling 28 fits\n",
      "[CV] n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False, total=  18.7s\n",
      "[CV] n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False, total=  18.9s\n",
      "[CV] n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False, total=  38.0s\n",
      "[CV] n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False, total=  38.6s\n",
      "[CV] n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False, total=  23.3s\n",
      "[CV] n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False, total= 1.1min\n",
      "[CV] n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False, total=  44.4s\n",
      "[CV] n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=17, min_samples_leaf=1, max_features=0.5, bootstrap=False, total= 1.4min\n",
      "[CV] n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False, total= 1.1min\n",
      "[CV] n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True \n",
      "[CV]  n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True, total=  12.5s\n",
      "[CV] n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=5, min_samples_leaf=17, max_features=0.45, bootstrap=False, total= 1.5min\n",
      "[CV] n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True \n",
      "[CV]  n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True, total=  26.3s\n",
      "[CV] n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True \n",
      "[CV]  n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True, total=   9.1s\n",
      "[CV] n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False, total= 1.1min\n",
      "[CV] n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True \n",
      "[CV]  n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True, total=  41.4s\n",
      "[CV] n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False \n",
      "[CV]  n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True, total=  19.0s\n",
      "[CV] n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False \n",
      "[CV]  n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True, total=  31.0s\n",
      "[CV] n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False, total=  19.9s\n",
      "[CV] n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=3, min_samples_leaf=11, max_features=0.5, bootstrap=False, total= 1.4min\n",
      "[CV] n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False \n",
      "[CV]  n_estimators=100, min_samples_split=12, min_samples_leaf=11, max_features=1.0, bootstrap=True, total=  54.6s\n",
      "[CV] n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False \n",
      "[CV]  n_estimators=100, min_samples_split=15, min_samples_leaf=3, max_features=0.05, bootstrap=True, total=  38.9s\n",
      "[CV] n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False, total=  38.5s\n",
      "[CV] n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False \n",
      "[CV]  n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False, total=  41.0s\n",
      "[CV]  n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False, total=  59.4s\n",
      "[CV]  n_estimators=200, min_samples_split=7, min_samples_leaf=5, max_features=0.15000000000000002, bootstrap=False, total= 1.2min\n",
      "[CV]  n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False, total= 1.2min\n",
      "[CV]  n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False, total= 1.4min\n",
      "[CV]  n_estimators=400, min_samples_split=11, min_samples_leaf=6, max_features=0.6500000000000001, bootstrap=False, total= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  28 out of  28 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=4),\n",
       "          error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=7, n_jobs=6,\n",
       "          param_distributions={'n_estimators': [100, 200, 400], 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=13, refit=True,\n",
       "          return_train_score='warn', scoring='r2', verbose=2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance_entropy = list(zip(train.drop(target, axis=1).columns.values, model.best_estimator_.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('item_activation_date_yday', 0.9306328561719555),\n",
       " ('item_activation_date_wday', 0.06936714382804414),\n",
       " ('item_activation_date_isholiday', 0.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(feat_importance_entropy, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00019463016228937757"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2604694388939306"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = np.sqrt(np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featurize test data and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.rename(columns={'activation_date': 'item_activation_date'})\n",
    "test = featurize_date_col(test, 'item_activation_date', remove_when_done=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.set_index('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['deal_probability'] = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test[['deal_probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('predictions/activation_date.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide predictions for train data to be used in ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['deal_probability'] = model.predict(train.drop('deal_probability', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['deal_probability']].to_csv('predictions/activation_date_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
